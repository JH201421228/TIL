# Lookahead Optimizer

소유자: 주헌 박

- **아이디어 한 줄**
    
    Lookahead는 “빠른 가중치(θ)”를 임의의 기존 옵티마이저(예: SGD/Adam)로 k번 업데이트한 뒤, “느린 가중치(φ)”를 θ 쪽으로 보간(φ ← φ + α(θ_k − φ))하여 학습을 안정화하고 분산을 낮추는 **래퍼 최적화기**입니다.
    
- **왜 쓰나 (직관 & 장점)**
    
    내부 루프(빠른 가중치)는 큰 학습률로 과감히 탐색하며 저곡률 방향으로 빠르게 전진하고, 외부 루프(느린 가중치)가 보간으로 진동을 완화해 **분산을 줄이고 안정성을 높입니다**. 과대 진동 없이 더 큰 학습률을 쓰기 쉬워 **하이퍼파라미터 민감도가 낮고** 수렴이 빨라집니다.
    
- **이론적 결과(요지)**
    
    노이즈가 있는 이차 모델에서, 같은 학습률로 비교 시 Lookahead의 **정상 상태 위험(steady-state risk)** 은 SGD보다 **항상 작다(분산 고정점이 더 낮다)** 는 고정점 해석을 제시합니다(정리 2). 즉 분산 감소가 수학적으로 뒷받침됩니다. 또한 동일한 최종 위험을 달성하도록 맞춘 설정들 중에서 Lookahead가 **더 빨리** 그 지점에 도달할 수 있음을 해석적/시뮬레이션으로 보입니다.
    
- **알고리즘 한눈에**
    1. 매 외부 스텝에서 θ를 φ로 동기화. 2) k개 미니배치로 임의 옵티마이저 A를 사용해 θ를 업데이트. 3) 외부 업데이트: φ ← φ + α(θ_k − φ). 4) 반복. (α: 보간 비율, k: 동기화 주기)
- **α(느린 가중치 스텝 크기) 선택**
    
    이차 근사를 쓰면 두 점 θ_0, θ_k 사이에서 손실을 최소화하는 **최적 α**의 폐형식을 유도할 수 있고, Fisher 대각 근사로 **적응적 α**를 추정해 클리핑하며 쓰는 방법을 제안합니다. 다만 **실무에선 고정 α가 간단하고 일반화도 더 낫다**고 보고합니다.
    
- **계산/메모리 오버헤드**
    
    매 외부 스텝에서의 파라미터 복사·보간만 추가되어 **연산 오버헤드는 O((k+1)/k)** 로 상수 수준이며, **가중치 한 벌 추가 저장**만 필요합니다.
    
- **실험 핵심 결과**
    - **CIFAR-10/100 & ImageNet**: ResNet-18/50/152에서 **수렴 가속**과 종종 **일반화 향상**을 관찰. ImageNet에서 공격적 스케줄과 조합 시 **50epoch에 Top-1 75.1%, 60epoch에 75.5%**, ResNet-152는 **49epoch에 77.0%, 60epoch에 77.96%** 달성.
    - **언어 모델링(PTB)**: LSTM에서 **훈련/검증/테스트 퍼플렉서티가 최상**(LA(Adam) 기준).
    - **기계번역(WMT14 En-De)**: Adam/AdaFactor 대비 **초기 수렴이 빠르고**, 최종 BLEU는 유사. 학습률 범위를 넓게 허용.
    - **로버스트니스**: 내부 옵티마이저의 **학습률/모멘텀을 바꿔도 일관된 이득**, k·α 변화에도 견고.
    - **모멘텀 상태 다루기**: 유지/보간/리셋 모두 SGD 대비 수렴 개선.
    - **SWA와의 관계**: SWA는 **후반 가중치 산술평균**으로 앙상블 유사 효과를 노리지만 시작 시점 선택이 민감. Lookahead는 **훈련 내내 EMA 기반 보간**으로 분산을 낮추며 상보적입니다. 조합 시 추가 향상도 관찰.
- **실무 체크리스트(요지)**
    
    설정이 단순합니다. **k=5, α=0.5~0.8**가 여러 벤치마크에서 잘 작동했고, **SGD/Adam에 한 줄로 래핑**해 적용 가능합니다.
    
- **한 줄 결론**
    
    Lookahead는 기존 옵티마이저의 업데이트 경로를 “미리 보고” 느린 EMA 경로로 수렴을 끌어당겨 **분산을 줄이고 하이퍼파라미터 민감도를 낮추며 수렴과 일반화를 동시에 개선**하는 경량 래퍼입니다.