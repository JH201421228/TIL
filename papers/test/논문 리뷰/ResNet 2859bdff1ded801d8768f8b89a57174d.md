# ResNet

소유자: 주헌 박

# ResNet 핵심 요약

1. **문제의식 (Degradation)**
- BN·좋은 초기화를 써도, **지름길 없는(plain) 깊은 네트**는 층을 더 쌓을수록 **훈련 오차가 커지는** 기현상(열화)이 나타남. “표현력↑”이 “최적화 난이도↑”에 져버리는 상황.
1. **핵심 아이디어 (Residual 학습)**
- 원하는 함수 (H(x))를 직접 맞추지 말고, **잔차** (F(x)=H(x)-x)를 학습.
- 출력은 **(y = F(x) + x)**. 여기서 (x)는 **항등 지름길(shortcut)**로 그냥 더해진다.
- 직관: 최적이 **항등**에 가깝다면, 복잡한 (H)를 맞추는 것보다 **(F)를 0에 수렴**시키는 게 훨씬 쉽다.
1. **구현 포인트 (Shortcut)**
- **항등 스킵**은 **추가 파라미터/연산량 ≈ 0**. 모양이 다를 때만 1×1 conv로 **프로젝션**(옵션 B)을 사용해 차원 맞춤.
- 덧셈 이후에 활성화(예: ReLU)를 두는 **post-activation** 구조(원 논문 기준).
1. **블록 설계**
- 얕은 모델/소형 데이터: **2층 잔차 블록**(3×3→3×3)이 심플하고 충분.
- 대형 모델(ImageNet): **병목 블록** **1×1→3×3→1×1**로 채널을 줄였다 복원해 **연산 효율**을 극대화. 스킵까지 무겁게 만들면 효율 손실이라 **항등 스킵**이 특히 중요.
1. **무엇이 쉬워졌나 (최적화 관점)**
- 스킵 경로가 **그라디언트 직통로**를 제공 → 깊어도 역전파가 건강하게 흐름.
- 실험적으로 각 층의 **응답 표준편차가 작고**, 깊어질수록 더 작아짐 → 각 층은 **미세 보정**만 수행, 전체는 그 미세 변화의 누적으로 강력한 표현력을 획득.
1. **스케일링 효과 (성능)**
- 동일 FLOPs에서 **ResNet ≫ plain**. 34층보다 **50/101/152층**이 **뚜렷이 더 정확**하며 열화가 사라짐.
- **ResNet-152**는 **VGG-16/19**보다 연산량이 적으면서 더 강함; 단일 모델 **top-5 4.49%**, 서로 다른 깊이 6개 **앙상블 3.57%**(ILSVRC 2015 1위).
1. **다운스트림 임팩트**
- 분류 백본을 **VGG → ResNet**으로 바꾸기만 해도 **Faster R-CNN**에서 VOC는 **mAP +3%p↑**, COCO의 엄격 지표 **mAP@[.5,.95] +6%p↑**. 분류·탐지·지역화·세그멘테이션 **모두 1위** 싹쓸이.
1. **실무 체크리스트**
- Conv→**BN**→ReLU 순서, SGD(모멘텀) + **스텝 감쇠** LR. 매우 깊을 땐 **LR 워밍업** 유용.
- 차원 증가 구간만 **프로젝션(옵션 B)**, 그 외는 **항등** 추천.
- 대형 모델은 **병목 블록** 필수. 탐지/세그에선 **BN 통계 고정**으로 메모리 세이브.
1. **오해 금지**
- 스킵 ≠ 하이웨이 게이팅: ResNet 스킵은 **무파라미터 항등**이 기본.
- **프로젝션은 필수 아님**(효율·공정 비교 위해 가급적 항등).
- **아주 깊다고 항상 더 좋진 않음**: CIFAR-10 같은 소형 데이터에 1000+층을 억지로 쓰면 **과적합** 가능—정규화/증강이 관건.

---

### 한 줄로 정리

**“입력에 ‘얼마나 보정할지’(잔차)만 쌓아서 배우면, 깊이가 커져도 학습이 쉬워지고 성능은 더 오른다.”**