# EfficientNet

소유자: 주헌 박

# 한 줄 핵심

하나만 키우지 말고, 깊이·너비·해상도를 **같이** 균형 있게 키워라—이를 위해 단일 계수 φ로 세 차원을 동시에 스케일하는 “compound scaling”을 제안하고, 이 방식으로 만든 EfficientNet-B0~B7이 정확도·파라미터·FLOPs 모두에서 기존 모델들을 크게 앞선다.

# 왜 중요한가

- 과거엔 깊이만(ResNet 계열), 너비만(WideResNet/모바일 계열), 혹은 입력 해상도만 키우는 식의 **단일 차원 스케일링**이 일반적이었는데, 이렇게 하면 성능이 금방 **포화**된다. 균형 잡힌 동시 스케일링이 필요하다는 게 저자들의 관찰이다.

# 무엇을 제안했나 (방법)

- **Compound scaling 공식**
    
    깊이 d=α^φ, 너비 w=β^φ, 해상도 r=γ^φ 로 함께 키우되, **α·β²·γ²≈2** 제약으로 FLOPs가 대략 2^φ 배로 늘게 조절한다. (Conv 연산량이 d, w², r²에 비례한다는 점을 활용)
    
- **Baseline + scaling 2단계**
    
    ① B0를 만들고 φ=1에서 작은 그리드 서치로 α,β,γ를 선정(α=1.2, β=1.1, γ=1.15) → ② φ만 늘려 B1~B7을 얻는다. 한 번의 소규모 탐색으로 전 패밀리를 뽑는 경제적 절차.
    

# 무엇을 만들었나 (모델)

- **EfficientNet-B0**: MBConv(Separable conv 기반 inverted bottleneck) + Squeeze-and-Excitation를 쓰는 모바일 친화 설계. 이후 compound scaling으로 B1~B7 생성.

# 성능 한눈에

- **ImageNet 단일-크롭 단일-모델**에서 EfficientNet-B7이 **Top-1 84.3%**를 달성하면서, 거대 모델 GPipe 대비 **파라미터 8.4×↓, 추론 6.1×↓**.
- 같은 정확도 대역에서 **파라미터·FLOPs를 한 자릿수 배수로 절감**(예: B4는 82.9%로 SENet 대비 파라미터 7.7×↓, FLOPs 10×↓).
- **전이 학습**에서도 8개 중 5개 데이터셋 SOTA 달성, 평균 파라미터 **9.6×↓**.

# 왜 잘 되나 (통찰)

- 해상도를 올리면 더 넓은 수용영역(깊이)과 더 미세한 패턴 포착(너비)이 **함께** 필요하다. 실제로 너비만 늘리면 금방 포화되지만, 깊이·해상도를 같이 높이면 같은 FLOPs에서도 이득이 커진다.
- 동일 FLOPs에서 **복합 스케일링이 단일 스케일링보다 최대 +2.5%p** 더 높았다(실험 곡선 및 CAM 시각화로 확인).

# 기억할 것 (실무 체크리스트)

1. 작은 모델에서 **α,β,γ를 한 번만** 찾아두고, 자원에 맞춰 φ만 조절해 **전체 라인업**을 만든다.
2. 연산량은 대략 d·w²·r²에 비례—즉 너비·해상도는 FLOPs 민감도가 높다. 설계 시 **예산 대비 정확도 곡선**을 항상 본다.