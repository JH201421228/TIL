# AlexNet

소유자: 주헌 박

# 📌 AlexNet 핵심 요약

---

## 1. 배경

- **문제**: 기존 객체 인식 방법(SIFT, HOG, Fisher Vectors 등)은 대규모 데이터에서 성능 한계.
- **기회**: ImageNet (120만 장, 1000 클래스) + GPU 발전 → 대규모 CNN 학습 가능.

---

## 2. 모델 구조 (Architecture)

- 총 **8개 층**:
    - **5 Conv (합성곱 층)**
    - **3 FC (완전연결 층)**
- 입력: 224×224 RGB 이미지
- 출력: 1000 클래스 Softmax
- 파라미터 수: **60M (6천만 개)**

---

## 3. 혁신적 기법 (Key Innovations)

1. **ReLU 활성화 함수**
    - 비포화, 빠른 학습 → tanh/sigmoid보다 훨씬 효과적.
2. **다중 GPU 학습**
    - 네트워크를 GPU 2개에 분산 → 더 큰 모델 학습 가능.
3. **LRN (Local Response Normalization)**
    - 뉴런 간 경쟁(측부 억제) 모방 → 일반화 성능 향상.
4. **Overlapping Pooling**
    - 풀링 창이 겹치도록 설정 → 과적합 감소.

---

## 4. 과적합 방지 기법

1. **Data Augmentation (데이터 증강)**
    - 무작위 자르기 + 좌우 반전.
    - 색상 강도 변형 (PCA 기반 RGB 변화).
2. **Dropout (드롭아웃)**
    - FC 층에서 뉴런을 확률적으로 끄기 (p=0.5).
    - 뉴런 간 공적응 방지 → 일반화 성능 ↑.

---

## 5. 학습 방법

- 최적화: **SGD + Momentum (0.9)**
- Weight decay: 0.0005 (정규화 효과 + 학습 안정화)
- 학습률 스케줄: 성능 향상 멈추면 LR을 1/10로 감소
- Epoch: 약 90회, 2개 GTX 580 GPU, 5~6일 학습

---

## 6. 성능 (Results)

- **ILSVRC-2010**: Top-5 오류 17.0% (기존 ~28%)
- **ILSVRC-2012**: Top-5 오류 15.3% (2위 26.2%) → **압도적 우승**
- **ImageNet-2009 (10k 클래스)**: Top-5 오류 40.9% (기존 60.9%)

---

## 7. 의의 (Discussion)

- CNN의 **깊이(Depth)** 자체가 성능의 핵심임을 증명.
- **사전학습 없이 지도학습만으로도 가능**함을 보임.
- 대규모 데이터셋 + GPU + CNN = 딥러닝 혁명의 시작.
- 이후 VGG, ResNet 등 더 깊고 정교한 네트워크로 발전.

---

# 🎯 결론

AlexNet의 핵심은

**“대규모 데이터셋과 GPU를 활용해, 깊은 CNN을 학습시킬 수 있다”**는 것을 세계에 처음으로 증명했다는 점입니다.

이 논문(2012)이 발표되면서, 컴퓨터 비전의 패러다임은 전통적 특징 기반 기법에서 **딥러닝 기반 CNN**으로 완전히 전환되었습니다.

---