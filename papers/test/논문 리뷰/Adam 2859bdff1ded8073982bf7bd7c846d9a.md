# Adam

소유자: 주헌 박

- **무엇을 제안하나?**Adam은 확률적 목적함수 최적화를 위해 1차 미분(gradient)만 쓰면서, 각 파라미터별로 **1차 모멘트(평균)**와 **2차 모멘트(비중심 분산)**의 이동평균을 추정해 **적응적 학습률**을 부여하는 옵티마이저입니다. 구현이 간단하고 메모리 요구가 작으며, **그라디언트 스케일 변화에 불변**, **비정상(non-stationary) 목적함수**와 **노이즈/희소 그라디언트**에도 강인하다는 점을 강조합니다. 기본 하이퍼파라미터도 대체로 손이 덜 갑니다.
- **핵심 아이디어(업데이트 규칙)**시간 `t`에서 그라디언트 `g_t`로부터 1차·2차 모멘트의 지수이동평균 `m_t`, `v_t`를 갱신하고, 초기 0에서 시작해 생기는 **편향(bias)**을 보정한 `m̂_t`, `v̂_t`로 업데이트합니다. 기본값으로 `α=0.001`, `β₁=0.9`, `β₂=0.999`, `ε=10⁻⁸`를 권장합니다. 업데이트는`θ_t = θ_{t-1} - α · m̂_t / (√v̂_t + ε)`
- **왜 '편향 보정'이 필요한가?**이동평균을 0에서 시작하면 초반엔 모멘트 추정치가 0 쪽으로 치우칩니다. 2차 모멘트의 경우 `E[v_t] = E[g_t²]·(1-β₂ᵗ) + ζ`가 되어 `(1-β₂ᵗ)`로 나눠 보정해야 초반 스텝이 과도해지지 않습니다(특히 `β₂`가 1에 가까운, 희소 그라디언트 상황).
- **스케일 불변성과 자동 감쇠(annealing)**그라디언트를 상수 `c`배 스케일해도 `(c·m̂_t) / √(c²·v̂_t) = m̂_t / √v̂_t`로 상쇄되어 **업데이트 크기가 스케일에 불변**입니다. 또한 신호-대-잡음비가 줄어드는 구간(예: 수렴 근처)에서 유효 스텝이 자연히 줄어드는 효과가 있습니다.
- **이론적 보장(온라인 컨벡스 최적화)**Adam은 온라인 컨벡스 설정에서 **후회도(regret) `R(T) = O(√T)`** 경계를 증명하며, 평균 후회도 `R(T)/T = O(1/√T)`로 0에 수렴합니다(학습률 `α_t ∝ t^(-1/2)`, `β₁,t` 지수감쇠 가정).
- **다른 방법과의 관계**Adam은 **AdaGrad**(희소 그라디언트에 강함)와 **RMSProp**(비정상 환경에 강함)의 장점을 결합한 설계입니다. RMSProp에는 초반 편향 보정이 없어 `β₂→1`에 가까운 설정에서 발산 위험이 커질 수 있고, AdaGrad는 `β₁=0`, `(1-β₂)→0` 극한과 `t^(-1/2)` 스케줄에서 Adam과 연결됩니다(보정이 있을 때).
- **AdaMax(변형)**`Lₚ` 규범 일반화를 `p→∞`로 보낸 변형으로, `u_t = max(β₂·u_{t-1}, |g_t|)`를 쓰는 **매우 간단하고 안정적인** 업데이트가 됩니다. 기본 권장값은 `α=0.002`, `β₁=0.9`, `β₂=0.999`.
- **실험 결과(요지)**로지스틱 회귀(MNIST), 다층퍼셉트론, CNN(CIFAR-10) 등에서 Adam은 **SGD+모멘텀**과 견줄 만큼 빠르거나 더 빠른 수렴을 보였고, 희소 특성(BoW)에서는 **AdaGrad 수준의 이점**을 유지했습니다. 또한 편향 보정이 없으면 특히 `β₂≈1`에서 학습이 불안정해짐을 VAE 실험으로 확인했습니다.
- **실무 한 줄 팁**기본값(`α=10⁻³`, `β₁=0.9`, `β₂=0.999`, `ε=10⁻⁸`)으로 시작하되, **희소/노이즈 큰 문제**와 **레이어별 스케일 차**가 큰 모델에서 특히 유용합니다. 초반 불안정 시엔 `β₂`를 살짝 낮추거나(예: 0.99), `ε`을 키우는 조정이 흔합니다.