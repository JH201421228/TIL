# Diffusion

소유자: 주헌 박

# 핵심 아이디어 한 줄 요약

- 데이터를 점점 “가우시안 잡음”으로 섞는 **정방향(diffusion) 과정**을 정의하고, 이를 **거꾸로 복원(역과정)**하도록 신경망을 학습해 고품질 샘플을 생성한다. 역과정은 **잡음(ε)을 예측**하도록 파라미터화하면 간단하고 성능이 좋으며, 이는 **denoising score matching + (annealed) Langevin dynamics**와 연결된다.

# 무엇을 했나

- **모델 구조**: 데이터 (x_0)에 타임스텝마다 작은 가우시안 노이즈(분산 (\beta_t))를 더해 (x_T)까지 보내는 고정된 **정방향 마르코프 체인**을 두고, 이를 **역으로 되돌리는 조건부 가우시안 체인** (p_\theta(x_{t-1}\mid x_t))을 학습. 역과정의 평균을 직접 예측하는 대신 **잡음 (\varepsilon)** 자체를 예측하도록 설정(“ε-prediction”). 샘플링은 **U-Net + self-attention** 백본을 T=1000스텝으로 수행.
- **학습 목적함수**: 표준 변분경계(ELBO)를 분석해, 실전에서는 더 간단한 **(L_{\text{simple}})**(모든 t에서 잡음 MSE)을 사용하면 **샘플 품질이 향상**됨을 보임.
- **결과**:
    - **CIFAR-10 (unconditional)** 에서 **FID 3.17**, **Inception Score 9.46** 달성(당시 SOTA 급).
    - **LSUN 256×256**에서도 ProgressiveGAN에 견줄만한 품질.
- **이론적 연결**: ε-prediction 파라미터화로 **score matching**과 **Langevin dynamics**와의 **등가성**을 드러냄—즉, “잡음이 섞인 데이터의 **점수(로그밀도 기울기)**를 학습 → 그 점수로 노이즈를 단계적으로 걷어내며 샘플링”이라는 그림을 정식화.
- **해석(압축)**: ELBO를 **진행형 손실 압축(progressive lossy coding)**으로 해석해, 역과정 스텝을 진행할수록 **거친 구조→미세 디테일**이 순차적으로 복원되는 **코스-투-파인** 생성 특성을 설명. 많은 비트가 **인지 불가능한 미세 왜곡**을 기술하는 데 쓰임도 보임.

# 왜 중요한가 (의의)

- **GAN/AR/Flow/VAE** 이외에, **확률적 역확산**이라는 경로로 **고품질 샘플**을 달성할 수 있음을 첫 대규모로 입증—이후 DDPM 계열이 텍스트-투-이미지 등 **현 SOTA 생성모델의 주류**가 되는 출발점.
- **학습은 안정적이고(없는 모드 붕괴), 목적함수는 단순**하며, **샘플은 고품질**이라는 실용적 조합을 제시.

# 키 포인트 10가지(실무 감각 위주)

1. **정방향(고정)**: (\beta_t) 선형 스케줄(작게), 데이터 스케일을 유지하도록 (\sqrt{1-\beta_t}) 계수 포함. 고정 q(·)로 ELBO의 일부가 상수 처리.
2. **역과정(학습)**: (\mu_\theta)를 직접 맞추기보다 **(\varepsilon_\theta(x_t,t))**를 예측하도록 재파라미터화—코드가 단순해지고 성능이 상승.
3. **목적함수 선택**: 이론적 ELBO보다 **(L_{\text{simple}})**이 **샘플 품질**에서 우위(특히 작은 t의 손실을 상대적으로 줄여 더 어려운 노이즈 구간에 집중).
4. **샘플링**: **Langevin-like 업데이트**(예측한 ε로 drift, 고정 (\sigma_t)로 noise). 구현·튜닝이 수월.
5. **아키텍처**: **U-Net + GroupNorm + 16×16 해상도에서 self-attention + sinusoidal time embedding**. 파라미터 공유로 모든 t에 동일 네트워크.
6. **성능 수치**: **CIFAR-10 FID 3.17 / IS 9.46**(train 기준 FID; test 기준 FID 5.24).
7. **우도(log-lik)**: 샘플 품질 대비 우도는 경쟁 모델 대비 낮은 편이지만, **눈에 보이지 않는 디테일** 기술에 많은 비트가 쓰인다는 **rate–distortion 분석** 제시.
8. **프로그레시브 생성**: 큰 형태→텍스처 순서로 나타남(중간 잠재 (x_t) 고정 시 **고수준 속성**이 공유됨).
9. **불안정성 회피**: 역과정 분산 (\Sigma_\theta)를 **학습**하는 건 불안정했고, **고정 분산**이 더 좋았다(논문 내 ablation).
10. **AR과의 연결**: 특정 설정에선 **일반화된 오토리그레시브 디코딩**으로 해석 가능—가우시안 노이즈 기반의 “비트 순서”로 생각할 수 있음.

# 한 문장씩 기억 포인트

- **Diffusion을 거꾸로 돌리는 네트워크**를 **잡음 예측(ε-prediction)** 방식과 **간단한 MSE 목표**로 학습하면, 안정적이고 강력한 **고품질 생성**이 가능해진다.