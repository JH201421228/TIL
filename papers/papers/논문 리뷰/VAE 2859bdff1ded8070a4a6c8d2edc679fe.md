# VAE

소유자: 주헌 박

# 한 줄 요약

연속 잠재변수가 있는 확률모형에서 난해한 사후분포를 빠르게 근사하기 위해, **재매개변수화 트릭**으로 미분 가능한 **변분 하한(ELBO)** 추정치(**SGVB**)를 만들고, 이를 **인코더 qφ(z|x)**(=인식/인퍼런스 모델)와 **디코더 pθ(x|z)**를 함께 학습하는 **AEVB** 알고리즘(=VAE)을 제안합니다.

# 왜 필요한가

- **난해함(Intractability)**: pθ(x)=∫pθ(z)pθ(x|z)dz, pθ(z|x) 모두 계산/미분이 어려운 경우가 흔하며(예: 비선형 신경망), 고전 VB의 평균장 기법도 적분이 막히는 경우가 많습니다.
- **대규모 데이터**: 배치 최적화가 비싸므로 미니배치/온라인 학습이 필요합니다. MCMC 같은 샘플링 기반 방법은 점별 반복샘플링 때문에 느립니다.
- 목표는 (1) θ의 근사 ML/MAP 추정, (2) z의 근사 사후추론, (3) x의 근사 주변추론(예: 노이즈제거/인페인팅/초해상도)입니다.

# 핵심 아이디어

1. **ELBO와 SGVB**
    
    각 데이터에 대해 로그 주변우도는 **KL + 하한(ELBO)**로 분해됩니다. ELBO는
    
    [
    
    \mathcal L(θ,φ;x)= -\mathrm{KL}(qφ(z|x)|pθ(z))+\mathbb E_{qφ(z|x)}[\log pθ(x|z)]
    
    ]
    
    형태이며, 첫 항은 정규화(규제), 둘째 항은 재구성(우도)입니다.
    
    나이브한 몬테카를로 그래디언트는 분산이 매우 커서 비실용적이므로, 저자들은 **SGVB**라는 낮은 분산의 미분가능 추정치를 제시합니다.
    
2. **재매개변수화 트릭**
    
    z∼qφ(z|x)를 **z=gφ(ε,x)**, ε∼p(ε)로 표현하면, 기대값의 몬테카를로 근사가 **φ에 대해 미분 가능**해집니다. 가우시안이면 z=μ+σ·ε가 전형적입니다.
    
    이를 ELBO에 적용하면 SGVB 추정식과 미니배치 학습 절차(Algorithm 1)가 나옵니다.
    
3. **AEVB(=VAE) 구조**
- **인코더 qφ(z|x)**: 데이터 x를 받아 잠재분포(보통 **대각 가우시안**)의 μ,σ를 출력.
- **디코더 pθ(x|z)**: z로부터 x의 분포(실수형은 가우시안, 이진은 베르누이)를 출력.
- **prior pθ(z)=N(0,I)**, **reparam**: z=μ+σ⊙ε, ε∼N(0,I).
1. **실용 팁**
    
    KL 항은 가우시안일 때 **닫힌형**으로 계산 가능해 분산이 더 낮은 ELBO 추정치(“L̃_B”)를 쓸 수 있습니다. 미니배치 기반 전체 ELBO 추정과 SGD/Adagrad로 최적화합니다.
    

# 무엇을 보여줬나(실험)

- **MNIST & Frey Face**에서 AEVB가 **Wake-Sleep**보다 하한을 더 빨리, 더 높게 최적화했습니다. 잠재차원을 늘려도 과적합이 두드러지지 않았는데, ELBO의 KL 규제 효과로 해석합니다.
- 낮은 잠재차원에서는 주변우도 추정(MCMC 기반)으로 **MCEM/HMC**와 비교했고, 온라인 학습이 어려운 MCEM 대비 AEVB의 장점을 보였습니다.

# 결론과 영향

- 제안한 **SGVB + AEVB(VAE)**는 연속 잠재변수 모델 전반에 적용 가능하며, **표준 확률적 경사법**으로 손쉽게 학습됩니다. 이후 **계층적/컨볼루션/시계열** 등으로 확장하는 로드맵을 제시합니다.

# 한 장 감각 요약

ELBO = **재구성 점수**(log pθ(x|z)) − **KL 규제**(q vs p).

인코더가 μ,σ를 내고, **z=μ+σ·ε**로 샘플해 디코더가 x를 복원. 이 구조를 **오토인코더처럼** 보되, 확률모형과 변분추론으로 **원리 있게** 학습하는 것이 VAE의 핵심입니다.