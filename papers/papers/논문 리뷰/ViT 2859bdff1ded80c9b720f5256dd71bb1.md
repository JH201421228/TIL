# ViT

소유자: 주헌 박

# 무엇을 했나

- **이미지를 16×16 패치로 잘라 토큰처럼 다루고, “순정” Transformer 인코더에 그대로 넣어 분류**했다. 패치 임베딩 + 위치 임베딩 + [CLS] 토큰을 사용하며, 아키텍처는 NLP의 Transformer와 거의 동일하다.
- **CNN 고유 편향(지역성·평행이동 등) 없이도**, **충분히 큰 데이터로 사전학습**하면 강력한 성능과 효율을 낼 수 있음을 실증했다.

# 왜 중요한가

- ViT는 **대규모 사전학습 → 소규모 다운스트림 미세조정**이라는 NLP 방식이 **비전에도 잘 통한다**는 것을 보여준다. **대규모 데이터에서 스케일링 효율**이 특히 좋다.

# 어떻게 했나 (방법 한 컷)

- 입력 이미지 → **패치 분할(예: 16×16)** → 각 패치 **선형투영(임베딩 D차원)** → **위치 임베딩** 더함 → **[CLS] 토큰**을 맨 앞에 붙여 **Transformer 인코더**에 통과 → **분류 헤드**. 고해상도 미세조정 시 **사전학습 위치임베딩을 2D 보간**해 길어진 시퀀스에 맞춘다.

# 무엇이 나왔나 (결과)

- **대규모 데이터(JFT-300M/Imagenet-21k)로 사전학습** 시, ImageNet/VTAB/CIFAR 등에서 **SOTA 수준** 또는 상회. 예) **ImageNet Top-1 88.55% (ViT-H/14, JFT)**, **VTAB 평균 77.63%** 등. **동일 성능 대비 예산(Compute)도 적게 듦**.
- **데이터가 작으면 CNN이 유리**, **데이터가 커질수록 ViT가 역전**. 즉, **‘인덕티브 바이어스 < 데이터 스케일’**이라는 메시지.
- **모델 스케일링**(Base/Large/Huge)에서 **깊이·시퀀스 길이(작은 패치)** 확대가 효과적이며, **하이브리드(CNN+ViT)**는 작은 예산에선 유리하지만 **큰 모델로 갈수록 순정 ViT와 격차가 줄어듦**.

# 세부 포인트 (실무에 도움 되는 것들)

- **미세조정 시 해상도 ↑**: 패치 크기는 고정, 시퀀스 길이만 증가. **위치임베딩 2D 보간**으로 호환. 일반적으로 **고해상도 미세조정이 이득**.
- **학습 설정**: 대규모 사전학습은 **Adam + 큰 weight decay(≈0.1)**, **워밍업+선형 감쇠**가 효과적. 미세조정은 **SGD+모멘텀**을 사용.
- **자기주의 시각화**: 낮은 층부터 **전역/국소 머리들이 공존**, 깊어질수록 **주의 거리(Receptive field 유사)가 증가**, 의미 있는 영역에 주의를 집중한다.
- **자기지도 사전학습(마스크드 패치 예측)**: **소폭 이득(≈+2%p)**은 있으나, 당시에는 **대규모 지도 사전학습이 더 강함**. (후속 연구 여지)

# 한계와 시사점

- **소데이터 학습 취약**: CNN의 귀납적 편향 부재로 **작은 데이터셋에선 과적합/일반화 저하**. 데이터·정규화·증강이 매우 중요.
- **분류 외 과제**(검출/분할 등) 확장과 **자기지도 대체**는 향후 과제로 명시. 이후 생태계(DeiT, Swin, MAE 등)의 촉매가 됨.

# 압축 한 줄

> 이미지를 ‘단어(패치) 시퀀스’로 보고, 순정 Transformer로 학습하면—충분히 큰 사전학습 데이터가 있을 때—CNN 없이도 SOTA를 찍고, 스케일-효율까지 잡을 수 있다.
>