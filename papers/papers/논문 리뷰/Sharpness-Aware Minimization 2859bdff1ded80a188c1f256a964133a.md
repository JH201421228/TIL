# Sharpness-Aware Minimization

소유자: 주헌 박

# 한눈에 보는 핵심

- **문제의식**: 거대 모델에서 단순히 학습 손실만 낮추면 일반화가 안 좋을 수 있다. 그래서 저자들은 **손실 값 + 손실의 날카로움(sharpness)**을 동시에 낮추는 학습법 **SAM(Sharpness-Aware Minimization)**을 제안한다. 요지는 “**이웃한 가중치 근방에서도 손실이 낮은 평평한(=덜 날카로운) 해**를 찾자”는 것.
- **핵심 수식(한 줄)**:
    
    (\displaystyle \min_w ; \max_{|\varepsilon|_p\le\rho} L_S(w+\varepsilon); (+ \lambda|w|_2^2))
    
    즉, “**가중치를 살짝 흔들어도 손실이 크게 안 늘어나는**” (w)를 찾는 미니맥스 최적화.
    
- **왜 통한다?**: 날카로운 최소점 대신 **완만한(평평한) 최소점**으로 유도되며, 이는 일반화와 경험적으로 연결된다. 헤시안 스펙트럼으로 보면, SAM은 최대 고유값과 스펙트럼 벌어짐(날카로움 지표)을 확 줄인다.
- **효과**: CIFAR-10/100, ImageNet, 파인튜닝 등 **광범위한 비전 과제에서 오차를 일관되게 낮춤**. 예: ImageNet에서 ResNet-152 Top-1 에러 **20.3% → 18.4%**.
- **부가 효용**: **라벨 노이즈에 강함**—복잡한 노이즈 전용 기법에 견줄 만큼 견고하다.

# 어떻게 동작하나(아주 짧게)

1. 현재 (w)에서 그라디언트 (\nabla L_S(w))로 **가장 손실이 잘 늘어나는 방향**의 미세 교란 (\hat\varepsilon(w))를 구한다(1차 테일러 근사와 쌍대 노름 해로 닫힌형 해를 갖는다).
2. 그 **교란된 가중치 (w+\hat\varepsilon)**에서 다시 그라디언트를 계산해 (w)를 업데이트한다.
    
    이렇게 하면 “흔들어도 안전한” 매끈한 해로 이동한다. 실전 구현은 **스텝당 그래디언트 계산 2번**이면 충분하다.
    

# 뭘 보여줬나(실험 하이라이트)

- **ImageNet**: ResNet-50/101/152 전 구간에서 SAM이 표준 학습 대비 Top-1/Top-5 에러를 낮춤. 특히 **ResNet-152의 Top-1 18.4%**는 표준 대비 큰 개선.
- **CIFAR-10/100**: 단순 WideResNet에도 SOTA 급 성능(예: **CIFAR-100 10.3%** with PyramidNet+ShakeDrop+SAM). 복잡한 모델·정규화 없이도 **일관된 이득**.
- **파인튜닝(전이학습)**: EfficientNet-b7/L2 파인튜닝에서 **CIFAR-10 0.30%, CIFAR-100 3.92%, ImageNet 11.39%** 등 SOTA 갱신 사례 보고.
- **라벨 노이즈 견고성**: CIFAR-10에서 20~80% 랜덤 라벨 플립 환경에서도 **SAM ≥ 최신 노이즈 대응법**(MentorMix에 근접/동급), 단순 부트스트랩과 조합 시 더욱 강건.

# 왜 평평함이 중요한가(직관)

날카로운 최소점은 **가중치가 아주 조금만 변해도 손실이 급증**한다. 데이터 셋이나 배치가 바뀌면 “같은 류의 변동”이 생기니, 테스트 분포에서 성능이 흔들리기 쉽다. 반면 평평한 최소점은 **근방 어디서나 손실이 낮아** 분포 변화나 잡음에 더 튼튼하다. 본 논문은 이를 **헤시안 고유값/스펙트럼**으로 직접 확인한다.

# SAM이 여는 추가 관점: m-sharpness

실전에서는 배치/가속기 단위로 SAM 업데이트를 계산한다. 이를 이론적으로 정리하며, **전체 데이터가 아닌 크기 (m)의 하위 묶음에서 본 날카로움—m-sharpness**를 정의한다. **작은 (m)일수록 일반화 예측력이 더 좋고, 실제 일반화도 개선**되는 경향을 보고한다. 대규모 데이터 병렬화와도 궁합이 좋다.

# 실무 체크리스트(바로 써먹기)

- **오버헤드**: 스텝당 그래디언트 2회라 비용은 늘지만, 보상(일반화 이득)이 큼. 대규모 병렬 학습에서도 잘 맞는다(각 가속기에서 서브배치로 독립 SAM 그라디언트 계산 후 평균).
- **하이퍼파라미터**: **(\rho)**는 기본값 **0.05**가 여러 실험에서 “튼튼한 디폴트”로 보고됨(추가 튜닝 없이도 성과).
- **조합성**: 데이터 증강, 레이블 스무딩, weight decay, 스케줄링 등 **기존 레시피 위에 얹어** 더 좋아지는 경향. 논문 전반의 벤치마크가 이를 뒷받침.

# 한 줄 결론

**SAM은 “손실의 크기”뿐 아니라 “손실 지형의 매끈함”을 같이 최적화해** 다양한 비전 태스크에서 **일반화를 꾸준히 개선**하고, **노이즈에도 강한** 실용적 최적화 테크닉이다.