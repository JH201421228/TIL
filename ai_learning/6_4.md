# MLE (Maximum Likelihood Estimation)  
**정의**: 최대우도추정(Maximum Likelihood Estimation; MLE)은 “관측된 데이터가 가장 그럴듯하게 나올” 파라미터를 고르는 방법.

**핵심 아이디어**: 데이터의 **우도(likelihood)**를 파라미터의 함수로 보고 그 값을 최대화한다(혹은 **음의 로그우도(NLL)**를 최소화).

**언제 쓰나**: 모든 딥러닝/통계 모델에서 손실 함수를 “가정한 확률분포”의 NLL로 연결해 해석하고 설계/비교/디버깅에 쓴다.

---

## A. 용어사전(Glossary)

- **우도(Likelihood)**: “데이터가 고정, 파라미터가 변수”인 함수. 분포 자체는 아님.
- **로그우도(Log-likelihood)**: 곱을 합으로 바꿔 계산/안정성을 높이려 로그를 취한 것.
- **NLL(Negative Log-Likelihood)**: −log ⁡likelihood. 최소화 대상 손실.
- **베르누이(Bernoulli) 분포**: 이진 라벨$y\in\{0,1\}$의 분포. $p(y)=q^y(1-q)^{1-y}$.
- **가우시안(Gaussian/Normal) 분포**: 실수 라벨 $y\in\mathbb{R}$의 연속 분포.
- **라플라스(Laplace) 분포**: 첨도가 큰 연속 분포. 절대오차(MAE)의 확률모형적 기원.
- **BCE(Binary Cross-Entropy)**: 베르누이 NLL. 이진분류 표준 손실.
- **MSE(Mean Squared Error)**: 가우시안 NLL(분산을 상수로 둘 때)과 비례하는 손실.
- **MAE(Mean Absolute Error)**: 라플라스 NLL과 비례하는 손실.
- **로짓(logit)**: 시그모이드 역함수. 확률 대신 로짓에서 계산하면 수치안정.

---

## B. 큰 그림(What/Why/How)

**What**

- 손실 함수 = “가정한 분포의 NLL”.
    - 이진분류: $y\sim \text{Bernoulli}(q_\mathbf{w}(x))$ → **BCE**
    - 회귀: $y\sim \mathcal{N}(\mu_\mathbf{w}(x),\sigma^2)$ → **MSE**(상수분산 가정)
    - 강한 이상치: $y\sim \text{Laplace}(\mu_\mathbf{w}(x),b)$ → **MAE**

**Why**

- 분포 가정이 **데이터 생성 가설**을 반영 → 손실 선택에 근거 제공.
- 같은 데이터라도 분포 가정이 다르면 최적화/일반화/민감도가 달라짐.

**How**

1. 분포를 가정한다.
2. 관측 $D=\{(x_i,y_i)\}_{i=1}^N$에 대한 우도 $L(\mathbf{w})=\prod_i p(y_i\mid x_i,\mathbf{w}).$
3. $\log L$을 최대화 $≡ \text{NLL}=-\sum_i \log p(y_i\mid x_i,\mathbf{w})$ 최소화.
4. 경사하강법으로 w를 학습.

**So what**

- “어떤 손실을 쓸까?”는 곧 “라벨의 **오차모형 분포**를 무엇으로 볼까?”라는 질문.