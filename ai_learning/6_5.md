# Softmax Regression  
**정의**: 소프트맥스 회귀(multinomial logistic regression)는 로짓(logits) z를 소프트맥스(softmax)로 확률 q로 바꾸고, **크로스 엔트로피(cross-entropy)**로 손실을 최소화해 **다중 분류(multiclass)**를 학습하는 선형 분류기.

**핵심 아이디어**:   $z=Wx+b →q_i = \exp(z_i)/\sum_j \exp(z_j) →L= -\log q_{y}L.$

$\partial L/\partial z = q - y$로 미분이 깔끔해 학습이 안정적.

**언제 쓰나**: 클래스가 **서로 배타적(Exactly one label)**일 때(예: 고양이/개/소 중 하나). 프레임워크에서 CrossEntropyLoss(PyTorch)는 **소프트맥스 내장**, **로짓을 그대로** 넣는다. 

---

## A. 용어사전 (Glossary)

- **로짓(logits)**: 마지막 선형층 출력 z(정규화 전 점수).
- **소프트맥스(softmax)**: $q_i=\frac{e^{z_i}}{\sum_j e^{z_j}}$ 확률 분포(양수, 합=1)로 변환. **수치적 안정화**를 위해 z에서 $\max z$를 빼서 계산.
- **크로스 엔트로피(cross-entropy)**: 타깃 분포 y와 예측 분포 q의 차이를 재는 손실. 원-핫 y에선 $L=-\log q_{y}$
- **다중 분류(multiclass)**: 하나의 샘플이 정확히 **1개 클래스**에 속하는 문제.
- **다중 레이블(multi-label)**: 여러 클래스가 **동시에 참**일 수 있는 문제. 각 클래스별 **시그모이드 + BCEWithLogitsLoss** 사용.

---

## B. 큰 그림 (What / Why / How)

### What

- 입력$x\in\mathbb{R}^{D}$를 받아 K개 클래스의 확률 $q\in\Delta^{K-1}$를 출력하는 선형 분류기: $z = Wx + b \quad (W\in\mathbb{R}^{K\times D},\ b\in\mathbb{R}^{K})$
    
    $q_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$
    
    $L(x,y) = -\sum_{i=1}^{K} y_i \log q_i = -\log q_{y}$
    
    소프트맥스 회귀는 **다항 로지스틱 회귀(multinomial logistic regression)**와 동치이며, 음의 로그가능도(NLL)가 곧 크로스 엔트로피다. 
    

### Why

- **확률 해석**: 출력이 확률분포(양수·합=1)라서 **의사결정/평가**가 명확.
- **미분 용이성**: $\partial L/\partial z = q-y$로 깔끔 → 효율적 역전파.
- **MLE 관점**: y가 **다항분포**를 따른다고 가정한 최대우도추정의 NLL 최소화가 **크로스 엔트로피 최소화**와 동일.

### How

1. **모델**: $z=Wx+b.$
2. **정규화**: 소프트맥스 $q=\mathrm{softmax}(z)$. 
3. **손실**: $L=-\log q_{y}$ (배치 평균). 
4. **최적화**: 경사하강법/Adam 등으로 W, b업데이트.
    - **그라디언트**: $\displaystyle \frac{\partial L}{\partial z}=q-y,\quad \frac{\partial L}{\partial W}=(q-y)\,x^{\top},\quad \frac{\partial L}{\partial b}=q-y$
5. **구현 팁**(PyTorch): nn.CrossEntropyLoss는 **내부에 log_softmax + NLLLoss 포함** ⇒ **모델에서 softmax 쓰지 말고** 로짓을 그대로 손실에 전달. 입력 shape은 [B, K], 타깃은 **정수 클래스 인덱스** [B]. 

### So what

- **배타적 클래스 분류**에 표준 해법.
- 간단한 선형결정경계로 부족하면 **CNN/Transformer** 등 표현력을 키워 **로짓 품질**을 끌어올린다. 손실/출력부는 동일.