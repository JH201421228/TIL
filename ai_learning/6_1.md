# 왜 이진 분류에서 sigmoid를 사용할까?  
- **정의**: 이진 분류(binary classification)는 입력 x에 대해 레이블 $y\in\{0,1\}$을 예측하는 문제.
- **핵심 아이디어**: 유닛 스텝 함수는 “결정만” 해주고 학습(미분)이 안 된다. Sigmoid는 **부드러운 확률 출력**과 **미분 가능성**을 제공해 **확률적 해석 + 경사하강법**을 가능케 한다.
- **언제 쓰나**: 출력이 두 클래스(예/아니오, 스팸/정상 등)인 모델의 **출력층**에서 확률(또는 점수)을 내고, **Binary Cross-Entropy**로 학습할 때.

---

## A. 용어사전(Glossary)

- **유닛 스텝 함수(Unit-step function)**: 입력이 0 이상이면 1, 아니면 0을 내는 비연속(미분 불가) 함수.
- **퍼셉트론(Perceptron)**: 은닉층 없이 선형 결합 후 스텝 함수로 이진 분류하는 고전 모델.
- **시그모이드(Sigmoid, logistic function)**: $σ(z)=\frac{1}{1+e^{-z}}$, 입력을 (0,1)로 눌러주는 S자 곡선. 확률처럼 해석 가능.
- **로지트(Logit)**: $logit(p)=\log\frac{p}{1-p}$. 확률 p를 (−∞,∞)로 펼친 값.
- **로지스틱 회귀(Logistic Regression)**: 선형 결합 $z=w⊤x+bz=w^\top x+b$에 시그모이드를 붙여 p(y=1∣x)p(y=1|x)를 모델링하고, **Binary Cross-Entropy**로 학습.
- **BCE(Binary Cross-Entropy)**: 이진 분류에서 쓰는 손실. $−[ylog⁡p^+(1−y)log⁡(1−p^)]$

---

## B. 큰 그림(What / Why / How)

### What: 문제와 모델

- **문제**: 키·몸무게 등 특징 $x∈{R}^d$로 “빼야 함(1) vs 찌워야 함(0)” 분류.
- **모델**: $z=w^\top x + b$ (선형 결합) → **스텝**을 쓰면 0/1만, **시그모이드**를 쓰면 $p^=σ(z)∈(0,1)\hat p=\sigma(z)\in(0,1).$

### Why: 스텝 함수의 한계와 Sigmoid의 장점

1. **미분 불가**: 스텝은 경사하강법(gradient descent) 불가 → 학습 곤란.
2. **과도한 경도(hard decision)**: 경계 근처 작은 변화에도 0↔1 급변, 불안정.
3. **Sigmoid의 해법**:
    - 전 구간 **미분 가능** → 경사하강법 OK.
    - **연속 확률** 출력 → “얼마나 1일 것 같은지”를 표현, 경계 선택도 더 합리적으로.

### How: 로지스틱 회귀로 학습

- 확률 모델: $p(y=1∣x)=σ(w⊤x+b)$
- 손실: **BCE**를 최소화(=우도 최대화).
- 경사하강법으로 $w, b$ 업데이트 → 선형 **결정경계** $w⊤x+b=0$을 학습.

---